import os
import git
import gym
import carla_gym
import inspect
import argparse
import numpy as np
import os.path as osp
from pathlib import Path
currentPath = osp.dirname(osp.abspath(inspect.getfile(inspect.currentframe())))
# sys.path.insert(1, currentPath + '/agents/stable_baselines/')
import shutil
import time

from stable_baselines.bench import Monitor
from stable_baselines.ddpg.policies import MlpPolicy as DDPGMlpPolicy
from stable_baselines.ddpg.policies import CnnPolicy as DDPGCnnPolicy
from stable_baselines.sac.policies import MlpPolicy as SACMlpPolicy
from stable_baselines.sac.policies import CnnPolicy as SACCnnPolicy
from stable_baselines.common.policies import MlpPolicy as CommonMlpPolicy
from stable_baselines.common.policies import MlpLstmPolicy as CommonMlpLstmPolicy
from stable_baselines.common.policies import CnnPolicy as CommonCnnPolicy
from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec
from stable_baselines import DDPG,SAC
from stable_baselines import PPO2
from stable_baselines import TRPO
from stable_baselines.deepq.policies import CnnPolicy as DQNCnnPolicy
from stable_baselines.deepq.policies import MlpPolicy as DQNMlpPolicy
from stable_baselines import A2C, DQN
from stable_baselines.common.policies import BasePolicy, nature_cnn, register_policy, sequence_1d_cnn, sequence_1d_cnn_ego_bypass_tc


#bdpl
from agents.reinforcement_learning.BDPL import BDPL, FeedForwardBoltzmannDistributionPolicy
from agents.reinforcement_learning.BDP_trpo import TRPO_bdp


from config import cfg, log_config_to_file, cfg_from_list, cfg_from_yaml_file


def parse_args_cfgs():
    parser = argparse.ArgumentParser()
    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for training')
    parser.add_argument('--env', help='environment ID', type=str, default='CarlaGymEnv-v1')
    parser.add_argument('--log_interval', help='Log interval (model)', type=int, default=100)
    parser.add_argument('--agent_id', type=int, default=None),
    parser.add_argument('--test_dir', type=str, default=None),
    parser.add_argument('--num_timesteps', type=float, default=1e7),
    parser.add_argument('--num_test_episode', type=int, default=2200)
    parser.add_argument('--save_path', help='Path to save trained model to', default=None, type=str)
    parser.add_argument('--log_path', help='Directory to save learning curve data.', default=None, type=str)
    parser.add_argument('--play_mode', type=int, help='Display mode: 0:off, 1:2D, 2:3D ', default=0)
    parser.add_argument('--verbosity', help='Terminal mode: 0:Off, 1:Action,Reward 2:All', default=0, type=int)
    parser.add_argument('--test', default=False, action='store_true')
    parser.add_argument('--env_change', help='test_mode, can be None or pertubation', default='None', type=str)
    parser.add_argument('--test_model', help='test model file name', type=str, default='')
    parser.add_argument('--test_last', help='test model best or last?', action='store_true', default=False)
    parser.add_argument('--carla_host', metavar='H', default='127.0.0.1', help='IP of the host server (default: 127.0.0.1)')
    parser.add_argument('-p', '--carla_port', metavar='P', default=2000, type=int, help='TCP port to listen to (default: 2000)')
    parser.add_argument('--tm_port', default=8000, type=int, help='Traffic Manager TCP port to listen to (default: 8000)')
    parser.add_argument('--carla_res', metavar='WIDTHxHEIGHT', default='1280x720', help='window resolution (default: 1280x720)')
    parser.add_argument('--learning_rate', type=float, metavar='lr', default='7e-4', help='learning_rate (default: 7e-4)')
#    parser.add_argument('--pg_lr_multiplyer', type=int, default=1, help='pg_loss learning rate multiplyer (currently deprecated)')
    parser.add_argument('--planner_mode', type=str, default='bdp', help='specify the mode of planner, can be bdp,. (default: bdp)')
    parser.add_argument('--is_finish_traj', type=int, default=1, help='whether in one env step, the car need to finish a whole trajectory. 1 means true. (default: 1)')
    parser.add_argument('--use_lidar', type=int, default=0, help='whether iuse lidar sensor for state representation, 1 means true. (default: 0)')
    parser.add_argument('--num_traj', type=int, default=3, help='number of candidate traj. generated by frenet (default: 3)')
    parser.add_argument('--scale_yaw', type=float, default='40', help='learning_rate (default: 40)')
    parser.add_argument('--scale_v', type=float, default='0.01', help='learning_rate (default: 0.01)')
    
    parser.add_argument('--bdp_debug', type=int, default='0', help='bdp_debug (default: 0)')
    parser.add_argument('--trpo_timesteps_per_batch', type=int, default='1024', help='trpo_timesteps_per_batch (default: 1024)')
    parser.add_argument('--short_hard', type=int, default='0', help='whether to use short hard mode, 1 is true (default: 0)')
    args = parser.parse_args()

    args.num_timesteps = int(args.num_timesteps)
    print(args.test_dir)
    if args.test and args.cfg_file is None:
        if args.agent_id is not None:
            path = 'logs/agent_{}/'.format(args.agent_id)
        elif args.test_dir is not None:
            path = 'logs/' + args.test_dir + '/'
        else:
            raise NotImplementedError
        conf_list = [cfg_file for cfg_file in os.listdir(path) if '.yaml' in cfg_file]
        args.cfg_file = path + conf_list[0]

    cfg_from_yaml_file(args.cfg_file, cfg)
    cfg.TAG = Path(args.cfg_file).stem
    cfg.EXP_GROUP_PATH = '/'.join(args.cfg_file.split('/')[1:-1])  # remove 'cfgs' and 'xxxx.yaml'

    # visualize all test scenarios
#    if args.test:
#        args.play_mode = True

    return args, cfg


if __name__ == '__main__':
    args, cfg = parse_args_cfgs()
    trpo_timesteps_per_batch = args.trpo_timesteps_per_batch
    print('Env is starting')
    env = gym.make(args.env, mode = args.planner_mode, is_finish_traj = args.is_finish_traj, use_lidar = args.use_lidar, num_traj = args.num_traj, scale_yaw = args.scale_yaw, scale_v = args.scale_v, debug = args.bdp_debug, short_hard = args.short_hard, env_change = args.env_change)
    if args.play_mode:
        env.enable_auto_render()
    env.begin_modules(args)
    if cfg.POLICY.NAME == 'DDPG':
        n_actions = env.action_space.shape[-1]  # ??? wull be scalar 1
    
    
#    localtime = time.strftime("%Y%m%d%H%M%S")
    # --------------------------------------------------------------------------------------------------------------------
    # --------------------------------------------------Training----------------------------------------------------------
    # --------------------------------------------------------------------------------------------------------------------
    if cfg.POLICY.NAME == 'DDPG':
        policy = {'MLP': DDPGMlpPolicy, 'CNN': DDPGCnnPolicy}   # DDPG does not have LSTM policy
    elif cfg.POLICY.NAME == 'SAC':
        policy = {'MLP': SACMlpPolicy, 'CNN': SACCnnPolicy}
    elif cfg.POLICY.NAME == 'DQN':
        policy = {'MLP': DQNMlpPolicy, 'CNN': DQNCnnPolicy}
    elif cfg.POLICY.NAME == 'BDP':
        policy = {'MLP': FeedForwardBoltzmannDistributionPolicy}
    elif cfg.POLICY.NAME == 'TRPO_BDP':
        policy = {'MLP': FeedForwardBoltzmannDistributionPolicy}
    else:
        policy = {'MLP': CommonMlpPolicy, 'LSTM': CommonMlpLstmPolicy, 'CNN': CommonCnnPolicy}

    if not args.test:  # training
        if args.agent_id is not None:
            # create log folder
            os.mkdir(currentPath + '/logs/agent_{}/'.format(args.agent_id))                             # create agent_id folder
            os.mkdir(currentPath + '/logs/agent_{}/models/'.format(args.agent_id))
            save_path = 'logs/agent_{}/models/'.format(args.agent_id)
            env = Monitor(env, 'logs/agent_{}/'.format(args.agent_id))    # logging monitor

            # log commit id
            repo = git.Repo(search_parent_directories=False)
            commit_id = repo.head.object.hexsha
            with open('logs/agent_{}/reproduction_info.txt'.format(args.agent_id), 'w') as f:  # Use file to refer to the file object
                f.write('Git commit id: {}\n\n'.format(commit_id))
                f.write('Program arguments:\n\n{}\n\n'.format(args))
                f.write('Configuration file:\n\n{}'.format(cfg))
                f.close()

            # save a copy of config file
            original_adr = currentPath + '/tools/cfgs_10vehicles/' + args.cfg_file.split('/')[-1]
            target_adr = currentPath + '/logs/agent_{}/'.format(args.agent_id) + args.cfg_file.split('/')[-1]
            shutil.copyfile(original_adr, target_adr)

        else:
            save_path = 'logs/'
            env = Monitor(env, 'logs/', info_keywords=('reserved',))  
        env.open_log_saver(currentPath + '/logs/agent_{}/'.format(args.agent_id), is_train = True)
        # logging monitor
        model_dir = save_path + '{}_final_model'.format(cfg.POLICY.NAME)                               # model save/load directory
        
        
        """
        note about the scale of input:, when feature_extraction is mlp, baselines won't scale the obs. When it is cnn, the 'scale' will be true.
            so, when using mlp, it is the user's responsibility to make the obs and acs input of proper scale.
            A good idea might be set env.observation_space bound by [0,1] or [-1,1]
            The original carla_gym is [-1,1]
        """
        
        n_steps_a2c = 5 #the nsteps for a2c. 5 is gym's default. bdp, TODO: add to config
        n_steps_bdp = 5
        n_steps_ppo = 5
        if cfg.POLICY.NAME == 'DDPG':
            action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions),
                                                        sigma=float(cfg.POLICY.ACTION_NOISE) * np.ones(n_actions))

            param_noise = AdaptiveParamNoiseSpec(initial_stddev=float(cfg.POLICY.PARAM_NOISE_STD),
                                                 desired_action_stddev=float(cfg.POLICY.PARAM_NOISE_STD))
            model = DDPG(policy[cfg.POLICY.NET], env, verbose=1, param_noise=param_noise, action_noise=action_noise, actor_lr = args.learning_rate,  tensorboard_log=save_path, policy_kwargs={'cnn_extractor': eval(cfg.POLICY.CNN_EXTRACTOR)})
        elif cfg.POLICY.NAME == 'SAC':
            model = SAC(policy[cfg.POLICY.NET], env, verbose=1, learning_rate = args.learning_rate, tensorboard_log=save_path, policy_kwargs={'cnn_extractor': eval(cfg.POLICY.CNN_EXTRACTOR)})
        elif cfg.POLICY.NAME == 'DQN':
            #note: DQN cannot work on contiuous action space env
            model = DQN(policy[cfg.POLICY.NET], env, verbose=1, learning_rate = args.learning_rate, tensorboard_log=save_path,  policy_kwargs={'cnn_extractor': eval(cfg.POLICY.CNN_EXTRACTOR)})
        elif cfg.POLICY.NAME == 'PPO2':
            model = PPO2(policy[cfg.POLICY.NET], env, verbose=1, learning_rate = args.learning_rate, n_steps = n_steps_ppo, model_dir=save_path,tensorboard_log=save_path, policy_kwargs={'cnn_extractor': eval(cfg.POLICY.CNN_EXTRACTOR)})
        elif cfg.POLICY.NAME == 'TRPO':
            model = TRPO(policy[cfg.POLICY.NET], env, verbose=1, timesteps_per_batch = trpo_timesteps_per_batch, model_dir=save_path,tensorboard_log=save_path, policy_kwargs={'cnn_extractor': eval(cfg.POLICY.CNN_EXTRACTOR)})#no learning_rate
        elif cfg.POLICY.NAME == 'A2C':
            model = A2C(policy[cfg.POLICY.NET], env, verbose=1, learning_rate = args.learning_rate, n_steps = n_steps_a2c, model_dir=save_path,tensorboard_log=save_path, policy_kwargs={'cnn_extractor': eval(cfg.POLICY.CNN_EXTRACTOR)})
        elif cfg.POLICY.NAME == 'TRPO_BDP':
            model = TRPO_bdp(policy[cfg.POLICY.NET], env, verbose=1, timesteps_per_batch = trpo_timesteps_per_batch, model_dir=save_path,tensorboard_log=save_path, policy_kwargs={'feature_extraction': 'mlp','cnn_extractor': eval(cfg.POLICY.CNN_EXTRACTOR)})#no learning_rate
        elif cfg.POLICY.NAME == 'BDP':
            model = BDPL(policy[cfg.POLICY.NET], env, verbose = 1, learning_rate = args.learning_rate, n_steps = n_steps_bdp, model_dir=save_path,tensorboard_log=save_path, policy_kwargs={'feature_extraction': 'mlp','cnn_extractor': eval(cfg.POLICY.CNN_EXTRACTOR)})
        else:
            print(cfg.POLICY.NAME)
            raise Exception('Algorithm name is not defined!')

        print('Model is Created')
        try:
            print('Training Started')
            if cfg.POLICY.NAME == 'DDPG':
                model.learn(total_timesteps=args.num_timesteps, log_interval=args.log_interval, save_path=save_path)
            else:
                model.learn(total_timesteps=args.num_timesteps, log_interval=args.log_interval)
        finally:
            print(100 * '*')
            print('FINISHED TRAINING; saving model...')
            print(100 * '*')
            # save model even if training fails because of an error
            model.save(model_dir)
            env.destroy()
            print('model has been saved.')

    # --------------------------------------------------------------------------------------------------------------------"""
    # ------------------------------------------------Test----------------------------------------------------------------"""
    # --------------------------------------------------------------------------------------------------------------------"""

    else:  # test
        if args.agent_id is not None:
            save_path = 'logs/agent_{}/models/'.format(args.agent_id)
            env = Monitor(env, 'logs/agent_{}/'.format(args.agent_id))
        elif args.test_dir is not None:
            save_path = 'logs/' + args.test_dir + '/models/'
            env = Monitor(env, 'logs/' + args.test_dir + '/test' + args.env_change)

        if args.test_model == '':
            best_last = 'best'
            if args.test_last:
                best_last = 'step'
            best_s = [int(best[5:-4])for best in os.listdir(save_path) if best_last in best]
            best_s.sort()
            args.test_model = best_last + '_{}'.format(best_s[-1])

        model_dir = save_path + args.test_model  # model save/load directory

        print('{} is Loading...'.format(args.test_model))
        if cfg.POLICY.NAME == 'DDPG':
            model = DDPG.load(model_dir)
            model.action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions),
                                                              sigma=np.zeros(n_actions))
            model.param_noise = None
        elif cfg.POLICY.NAME == 'PPO2':
            model = PPO2.load(model_dir)
        elif cfg.POLICY.NAME == 'SAC':
            model = SAC.load(model_dir)
        elif cfg.POLICY.NAME == 'TRPO':
            model = TRPO.load(model_dir)
        elif cfg.POLICY.NAME == 'A2C':
            model = A2C.load(model_dir)
        elif cfg.POLICY.NAME == 'TRPO_BDP':
            model = TRPO_bdp.load(model_dir)
        else:
            print(cfg.POLICY.NAME)
            raise Exception('Algorithm name is not defined!')

        print('Model is loaded')
        try:
            obs = env.reset()
            tmp_cnt = 0
            while True:
                if tmp_cnt >= args.num_test_episode:
                    break
                
                if cfg.POLICY.NAME == 'TRPO_BDP' or cfg.POLICY.NAME == 'BDP':
                    obs = np.array(obs)
                    ac_candidates,n_ac_candidates = env.external_sampler()
                    action, vpred, _states, prob, goodness = model.step(obs.reshape(-1, *obs.shape),ac_candidates, None, None, debug1=True)
#                    action = action[0][0]
#                    print('max:',np.argmax(goodness),' sampling:',action[0][0])
                    action = np.argmax(goodness)
                    # remarks: in bdp, the action is actually action_idx, does not need clipping
                else:
                    action, _states = model.predict(obs)
                obs, rewards, done, info = env.step(action)
                
                if args.play_mode != 0:
                    env.render()
                if done:
                    obs = env.reset()
                    tmp_cnt += 1
                    print(tmp_cnt,'/',args.num_test_episode,' finished')
        finally:
            env.destroy()
